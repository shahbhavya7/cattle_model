{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "432985f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37b5c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../Data/Raw/cattle.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd2996b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlations with ActualBodyWeight:\n",
      "ActualBodyWeight        1.000000\n",
      "HeartGirth              0.859868\n",
      "AbdGirth                0.851238\n",
      "ChestDepth              0.651687\n",
      "Scapuloischiallength    0.563804\n",
      "WHHeightAtWither        0.524599\n",
      "Rumpheight              0.516874\n",
      "BLBodylengthcm          0.477655\n",
      "RumpWidth               0.439753\n",
      "HockBoneDiameter        0.410467\n",
      "RumpLength              0.368583\n",
      "CannonBoneDiameter      0.336118\n",
      "SternumHeight          -0.015735\n",
      "AnimalNo               -0.054934\n",
      "Name: ActualBodyWeight, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "correlations = df.corr()[\"ActualBodyWeight\"].sort_values(ascending=False)\n",
    "\n",
    "print(\"Correlations with ActualBodyWeight:\")\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72fb1625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x=correlations.index[:-1], y=correlations.values[:-1])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Feature Correlations with Actual Body Weight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3abef645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update your feature set based on correlation analysis\n",
    "selected_features = [\n",
    "    \"HeartGirth\", \"AbdGirth\", \"ChestDepth\", \"Scapuloischiallength\",\n",
    "    \"WHHeightAtWither\", \"Rumpheight\", \"BLBodylengthcm\", \"RumpWidth\",\n",
    "    \"HockBoneDiameter\", \"RumpLength\"\n",
    "]\n",
    "\n",
    "X = df[selected_features]\n",
    "y = df[\"ActualBodyWeight\"]\n",
    "\n",
    "# Proceed with preprocessing and model building as before\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d12a2b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import layers, models, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d70e423",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28161bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93863855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gauta\\OneDrive\\Dokumentai\\cattle_model\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)  # output layer for regression\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e576fbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7e3f74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 366402.8438 - mae: 600.6954 - val_loss: 379876.0938 - val_mae: 611.0637\n",
      "Epoch 2/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 364584.9062 - mae: 599.2001 - val_loss: 377434.1875 - val_mae: 609.1299\n",
      "Epoch 3/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 361785.4375 - mae: 596.8785 - val_loss: 373546.0625 - val_mae: 606.0303\n",
      "Epoch 4/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 357232.2812 - mae: 593.0803 - val_loss: 367153.1250 - val_mae: 600.9030\n",
      "Epoch 5/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 350006.7812 - mae: 586.9838 - val_loss: 356893.2500 - val_mae: 592.5822\n",
      "Epoch 6/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 338726.9062 - mae: 577.3802 - val_loss: 341723.5938 - val_mae: 580.0099\n",
      "Epoch 7/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 322510.9688 - mae: 563.1832 - val_loss: 320348.7500 - val_mae: 561.7161\n",
      "Epoch 8/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 300471.3125 - mae: 543.1633 - val_loss: 292905.4062 - val_mae: 537.0764\n",
      "Epoch 9/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 273411.9688 - mae: 517.0699 - val_loss: 259556.7500 - val_mae: 504.8177\n",
      "Epoch 10/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 241569.3125 - mae: 483.4845 - val_loss: 222296.6719 - val_mae: 464.8878\n",
      "Epoch 11/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 207354.7656 - mae: 443.7306 - val_loss: 184397.8281 - val_mae: 418.1159\n",
      "Epoch 12/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 173382.2812 - mae: 399.3405 - val_loss: 149798.0312 - val_mae: 366.7727\n",
      "Epoch 13/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 143479.1094 - mae: 354.8171 - val_loss: 121481.8672 - val_mae: 314.9185\n",
      "Epoch 14/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 119901.3203 - mae: 319.6302 - val_loss: 101575.9609 - val_mae: 280.8157\n",
      "Epoch 15/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 103041.8672 - mae: 293.4034 - val_loss: 88987.3047 - val_mae: 264.8471\n",
      "Epoch 16/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 91622.4531 - mae: 272.7964 - val_loss: 81445.4062 - val_mae: 253.8015\n",
      "Epoch 17/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 83965.3281 - mae: 259.1268 - val_loss: 76315.6172 - val_mae: 245.8158\n",
      "Epoch 18/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 78276.3984 - mae: 248.0606 - val_loss: 71864.3281 - val_mae: 238.2519\n",
      "Epoch 19/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 73136.4766 - mae: 238.7637 - val_loss: 68108.3828 - val_mae: 231.8965\n",
      "Epoch 20/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 68637.5547 - mae: 230.1334 - val_loss: 64372.3359 - val_mae: 225.6521\n",
      "Epoch 21/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 64330.5039 - mae: 221.3980 - val_loss: 61242.7227 - val_mae: 220.6264\n",
      "Epoch 22/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 60026.8008 - mae: 213.2132 - val_loss: 57564.2891 - val_mae: 214.0667\n",
      "Epoch 23/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 55973.7266 - mae: 204.6586 - val_loss: 54209.6562 - val_mae: 207.8709\n",
      "Epoch 24/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 52054.8125 - mae: 196.8133 - val_loss: 50787.6523 - val_mae: 201.3239\n",
      "Epoch 25/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 48221.9531 - mae: 188.6864 - val_loss: 47759.0352 - val_mae: 195.1536\n",
      "Epoch 26/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 44717.4883 - mae: 180.7683 - val_loss: 44555.4805 - val_mae: 188.4795\n",
      "Epoch 27/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 41204.8398 - mae: 173.1161 - val_loss: 41671.3281 - val_mae: 182.0421\n",
      "Epoch 28/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 37921.8984 - mae: 165.1834 - val_loss: 38770.9961 - val_mae: 175.3733\n",
      "Epoch 29/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 34912.1953 - mae: 157.4668 - val_loss: 36348.7969 - val_mae: 169.2525\n",
      "Epoch 30/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 32170.9629 - mae: 151.0859 - val_loss: 33736.0352 - val_mae: 163.0347\n",
      "Epoch 31/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 29471.7207 - mae: 143.7610 - val_loss: 31046.6484 - val_mae: 156.0413\n",
      "Epoch 32/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 26961.1484 - mae: 136.8408 - val_loss: 28971.0098 - val_mae: 150.3894\n",
      "Epoch 33/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 24646.9570 - mae: 130.7339 - val_loss: 26594.8398 - val_mae: 143.8041\n",
      "Epoch 34/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 22484.1309 - mae: 124.0678 - val_loss: 24866.4180 - val_mae: 138.2373\n",
      "Epoch 35/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 20462.9824 - mae: 118.2400 - val_loss: 22569.1367 - val_mae: 131.5205\n",
      "Epoch 36/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 18692.5410 - mae: 112.6153 - val_loss: 20990.0801 - val_mae: 126.1747\n",
      "Epoch 37/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 17056.9043 - mae: 107.3165 - val_loss: 19517.0469 - val_mae: 121.0916\n",
      "Epoch 38/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 15562.2520 - mae: 102.0604 - val_loss: 17998.0371 - val_mae: 115.7323\n",
      "Epoch 39/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14250.8564 - mae: 97.3604 - val_loss: 16615.9062 - val_mae: 110.8822\n",
      "Epoch 40/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13078.6572 - mae: 92.8785 - val_loss: 15294.9336 - val_mae: 105.7690\n",
      "Epoch 41/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 11917.1104 - mae: 88.1406 - val_loss: 14375.4609 - val_mae: 101.9060\n",
      "Epoch 42/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 10948.9482 - mae: 84.2488 - val_loss: 13488.3564 - val_mae: 98.0956\n",
      "Epoch 43/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 10051.3594 - mae: 80.6685 - val_loss: 12457.6914 - val_mae: 93.7300\n",
      "Epoch 44/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9258.0615 - mae: 77.4069 - val_loss: 11607.8311 - val_mae: 89.8170\n",
      "Epoch 45/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8558.2305 - mae: 74.1285 - val_loss: 10769.0059 - val_mae: 85.5731\n",
      "Epoch 46/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7893.1650 - mae: 71.2069 - val_loss: 10080.2900 - val_mae: 82.3781\n",
      "Epoch 47/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7313.2964 - mae: 68.6779 - val_loss: 9508.4570 - val_mae: 79.2697\n",
      "Epoch 48/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6784.7788 - mae: 66.0843 - val_loss: 8840.2314 - val_mae: 75.7165\n",
      "Epoch 49/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6329.5986 - mae: 63.6848 - val_loss: 8454.4229 - val_mae: 73.0476\n",
      "Epoch 50/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5887.2090 - mae: 61.5773 - val_loss: 7703.2207 - val_mae: 69.5110\n",
      "Epoch 51/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5524.4453 - mae: 59.8748 - val_loss: 7203.1748 - val_mae: 66.7570\n",
      "Epoch 52/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5165.8140 - mae: 57.9470 - val_loss: 6925.5996 - val_mae: 64.5293\n",
      "Epoch 53/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4876.6030 - mae: 56.3040 - val_loss: 6515.4795 - val_mae: 62.0383\n",
      "Epoch 54/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4576.6528 - mae: 54.6314 - val_loss: 6140.0566 - val_mae: 59.9984\n",
      "Epoch 55/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4347.8267 - mae: 53.2745 - val_loss: 5744.4180 - val_mae: 57.7348\n",
      "Epoch 56/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4114.1987 - mae: 51.9841 - val_loss: 5548.0122 - val_mae: 56.5600\n",
      "Epoch 57/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3903.3076 - mae: 50.7107 - val_loss: 5199.9194 - val_mae: 54.4685\n",
      "Epoch 58/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3724.9424 - mae: 49.4677 - val_loss: 4967.8242 - val_mae: 52.9281\n",
      "Epoch 59/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3564.0071 - mae: 48.4714 - val_loss: 4747.1235 - val_mae: 51.5134\n",
      "Epoch 60/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3402.8110 - mae: 47.3583 - val_loss: 4518.5596 - val_mae: 49.9767\n",
      "Epoch 61/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3260.3745 - mae: 46.3382 - val_loss: 4306.1289 - val_mae: 48.5720\n",
      "Epoch 62/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3137.6870 - mae: 45.4296 - val_loss: 4128.9727 - val_mae: 47.2987\n",
      "Epoch 63/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3019.5747 - mae: 44.5793 - val_loss: 3991.9827 - val_mae: 46.4456\n",
      "Epoch 64/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2914.2698 - mae: 43.7740 - val_loss: 3810.8547 - val_mae: 45.3451\n",
      "Epoch 65/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2807.0908 - mae: 42.9421 - val_loss: 3679.0325 - val_mae: 44.3724\n",
      "Epoch 66/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2720.2463 - mae: 42.3085 - val_loss: 3538.7327 - val_mae: 43.4125\n",
      "Epoch 67/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2638.0110 - mae: 41.6051 - val_loss: 3425.7341 - val_mae: 42.4639\n",
      "Epoch 68/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2553.1936 - mae: 40.9068 - val_loss: 3328.2776 - val_mae: 41.6747\n",
      "Epoch 69/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2482.4407 - mae: 40.2610 - val_loss: 3147.1340 - val_mae: 40.3529\n",
      "Epoch 70/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2414.5293 - mae: 39.6637 - val_loss: 3102.4314 - val_mae: 39.8912\n",
      "Epoch 71/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2348.4761 - mae: 39.0676 - val_loss: 2990.9346 - val_mae: 39.1379\n",
      "Epoch 72/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2285.4038 - mae: 38.5968 - val_loss: 2896.3284 - val_mae: 38.5755\n",
      "Epoch 73/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2226.1033 - mae: 38.0635 - val_loss: 2819.4717 - val_mae: 38.0326\n",
      "Epoch 74/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2173.3721 - mae: 37.5624 - val_loss: 2737.0671 - val_mae: 37.2044\n",
      "Epoch 75/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2124.7349 - mae: 37.2311 - val_loss: 2680.9734 - val_mae: 37.0852\n",
      "Epoch 76/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2067.6677 - mae: 36.5982 - val_loss: 2576.9734 - val_mae: 36.1685\n",
      "Epoch 77/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2019.1288 - mae: 36.1946 - val_loss: 2539.5442 - val_mae: 35.8225\n",
      "Epoch 78/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1983.0797 - mae: 35.8619 - val_loss: 2492.3311 - val_mae: 35.5280\n",
      "Epoch 79/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1946.2046 - mae: 35.4448 - val_loss: 2355.3918 - val_mae: 34.5920\n",
      "Epoch 80/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1887.7504 - mae: 34.9343 - val_loss: 2381.5276 - val_mae: 34.7331\n",
      "Epoch 81/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1855.8708 - mae: 34.6223 - val_loss: 2314.9470 - val_mae: 34.1274\n",
      "Epoch 82/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1822.0371 - mae: 34.2995 - val_loss: 2230.9773 - val_mae: 33.6164\n",
      "Epoch 83/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1797.6328 - mae: 34.0747 - val_loss: 2204.0562 - val_mae: 33.5058\n",
      "Epoch 84/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1762.7544 - mae: 33.7369 - val_loss: 2124.5203 - val_mae: 32.8653\n",
      "Epoch 85/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1720.4373 - mae: 33.2790 - val_loss: 2135.2903 - val_mae: 32.7414\n",
      "Epoch 86/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1697.8811 - mae: 33.0256 - val_loss: 2067.6448 - val_mae: 32.2893\n",
      "Epoch 87/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1667.2278 - mae: 32.6798 - val_loss: 2000.5446 - val_mae: 31.9851\n",
      "Epoch 88/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1639.2686 - mae: 32.4621 - val_loss: 2011.9254 - val_mae: 31.8022\n",
      "Epoch 89/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1617.3890 - mae: 32.1812 - val_loss: 1941.0563 - val_mae: 31.4204\n",
      "Epoch 90/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1587.7214 - mae: 31.8856 - val_loss: 1914.8838 - val_mae: 31.2359\n",
      "Epoch 91/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1572.8484 - mae: 31.6739 - val_loss: 1853.4677 - val_mae: 30.6586\n",
      "Epoch 92/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1541.8043 - mae: 31.4071 - val_loss: 1881.3035 - val_mae: 30.6940\n",
      "Epoch 93/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1517.6006 - mae: 31.1137 - val_loss: 1795.3477 - val_mae: 30.2857\n",
      "Epoch 94/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1503.6289 - mae: 30.8914 - val_loss: 1753.5133 - val_mae: 30.0489\n",
      "Epoch 95/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1475.7169 - mae: 30.7146 - val_loss: 1768.9829 - val_mae: 29.8479\n",
      "Epoch 96/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1463.5546 - mae: 30.5062 - val_loss: 1701.2296 - val_mae: 29.4584\n",
      "Epoch 97/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1445.9240 - mae: 30.3606 - val_loss: 1722.8739 - val_mae: 29.3981\n",
      "Epoch 98/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1423.0391 - mae: 30.1161 - val_loss: 1666.6112 - val_mae: 29.0732\n",
      "Epoch 99/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1408.1375 - mae: 29.9657 - val_loss: 1588.6660 - val_mae: 28.7023\n",
      "Epoch 100/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1390.0842 - mae: 29.8248 - val_loss: 1634.9458 - val_mae: 28.6809\n",
      "Epoch 101/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1365.7638 - mae: 29.5781 - val_loss: 1603.2251 - val_mae: 28.5498\n",
      "Epoch 102/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1347.8253 - mae: 29.3102 - val_loss: 1540.4242 - val_mae: 28.1343\n",
      "Epoch 103/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1332.6526 - mae: 29.2995 - val_loss: 1551.2556 - val_mae: 28.0844\n",
      "Epoch 104/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1314.0605 - mae: 29.0275 - val_loss: 1511.7091 - val_mae: 27.6679\n",
      "Epoch 105/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1301.9749 - mae: 28.8694 - val_loss: 1500.7898 - val_mae: 27.6552\n",
      "Epoch 106/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1285.4437 - mae: 28.7228 - val_loss: 1470.1530 - val_mae: 27.3176\n",
      "Epoch 107/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1271.4056 - mae: 28.5412 - val_loss: 1452.4207 - val_mae: 27.2846\n",
      "Epoch 108/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1265.2892 - mae: 28.4546 - val_loss: 1431.4020 - val_mae: 26.9936\n",
      "Epoch 109/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1239.7058 - mae: 28.1987 - val_loss: 1444.0442 - val_mae: 26.9703\n",
      "Epoch 110/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1232.9723 - mae: 28.2704 - val_loss: 1405.8040 - val_mae: 26.8215\n",
      "Epoch 111/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1218.8354 - mae: 27.9984 - val_loss: 1383.1951 - val_mae: 26.4205\n",
      "Epoch 112/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1207.8499 - mae: 27.8424 - val_loss: 1373.5609 - val_mae: 26.3407\n",
      "Epoch 113/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1195.6852 - mae: 27.7691 - val_loss: 1364.9553 - val_mae: 26.3417\n",
      "Epoch 114/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1181.9221 - mae: 27.6122 - val_loss: 1333.7031 - val_mae: 26.0613\n",
      "Epoch 115/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1170.9359 - mae: 27.5223 - val_loss: 1321.2292 - val_mae: 25.8601\n",
      "Epoch 116/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1163.1591 - mae: 27.4864 - val_loss: 1325.1521 - val_mae: 25.7148\n",
      "Epoch 117/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1149.3771 - mae: 27.2917 - val_loss: 1297.0111 - val_mae: 25.6282\n",
      "Epoch 118/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1136.8457 - mae: 27.1249 - val_loss: 1282.6075 - val_mae: 25.6184\n",
      "Epoch 119/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1129.7260 - mae: 27.1169 - val_loss: 1282.4044 - val_mae: 25.2848\n",
      "Epoch 120/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1115.8949 - mae: 26.9235 - val_loss: 1246.8167 - val_mae: 25.0012\n",
      "Epoch 121/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1108.6414 - mae: 26.8763 - val_loss: 1227.8004 - val_mae: 25.0796\n",
      "Epoch 122/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1103.3990 - mae: 26.8205 - val_loss: 1235.4225 - val_mae: 24.9054\n",
      "Epoch 123/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1090.6554 - mae: 26.6094 - val_loss: 1206.8254 - val_mae: 24.6561\n",
      "Epoch 124/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1086.8610 - mae: 26.5558 - val_loss: 1191.3477 - val_mae: 24.7550\n",
      "Epoch 125/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1073.6385 - mae: 26.5027 - val_loss: 1200.0005 - val_mae: 24.5747\n",
      "Epoch 126/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1063.4498 - mae: 26.3819 - val_loss: 1181.7244 - val_mae: 24.5854\n",
      "Epoch 127/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1053.2019 - mae: 26.2182 - val_loss: 1179.0944 - val_mae: 24.4425\n",
      "Epoch 128/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1047.5515 - mae: 26.2335 - val_loss: 1161.5464 - val_mae: 24.2978\n",
      "Epoch 129/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1040.7694 - mae: 26.1124 - val_loss: 1154.3949 - val_mae: 24.1195\n",
      "Epoch 130/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1035.0726 - mae: 26.0132 - val_loss: 1155.1050 - val_mae: 24.0317\n",
      "Epoch 131/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1030.4415 - mae: 26.0182 - val_loss: 1125.5208 - val_mae: 24.2259\n",
      "Epoch 132/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1019.0005 - mae: 25.8465 - val_loss: 1136.7462 - val_mae: 23.9257\n",
      "Epoch 133/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1009.0445 - mae: 25.7846 - val_loss: 1117.5393 - val_mae: 23.7439\n",
      "Epoch 134/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1002.7014 - mae: 25.7044 - val_loss: 1115.4427 - val_mae: 23.7241\n",
      "Epoch 135/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 992.9993 - mae: 25.6709 - val_loss: 1108.8987 - val_mae: 23.6690\n",
      "Epoch 136/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 992.6354 - mae: 25.6226 - val_loss: 1094.1376 - val_mae: 23.6439\n",
      "Epoch 137/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 985.0968 - mae: 25.4986 - val_loss: 1083.4866 - val_mae: 23.3834\n",
      "Epoch 138/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 977.1107 - mae: 25.4647 - val_loss: 1090.7863 - val_mae: 23.3974\n",
      "Epoch 139/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 968.4495 - mae: 25.3238 - val_loss: 1074.9312 - val_mae: 23.3556\n",
      "Epoch 140/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 967.0364 - mae: 25.2511 - val_loss: 1071.6171 - val_mae: 23.3538\n",
      "Epoch 141/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 958.4568 - mae: 25.2419 - val_loss: 1074.5729 - val_mae: 23.3181\n",
      "Epoch 142/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 957.8406 - mae: 25.2688 - val_loss: 1045.3810 - val_mae: 23.3790\n",
      "Epoch 143/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 951.9791 - mae: 25.0847 - val_loss: 1022.6973 - val_mae: 23.1496\n",
      "Epoch 144/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 940.1399 - mae: 25.0111 - val_loss: 1057.3489 - val_mae: 22.9007\n",
      "Epoch 145/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 938.5950 - mae: 24.9126 - val_loss: 1041.4139 - val_mae: 23.0147\n",
      "Epoch 146/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 941.2686 - mae: 25.0773 - val_loss: 1024.3180 - val_mae: 22.7698\n",
      "Epoch 147/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 929.1312 - mae: 24.8432 - val_loss: 1008.1130 - val_mae: 22.9237\n",
      "Epoch 148/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 922.9830 - mae: 24.7987 - val_loss: 1020.3870 - val_mae: 22.9092\n",
      "Epoch 149/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 922.0560 - mae: 24.7670 - val_loss: 1007.4315 - val_mae: 22.6925\n",
      "Epoch 150/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 929.9500 - mae: 24.9865 - val_loss: 1015.8290 - val_mae: 22.8203\n",
      "Epoch 151/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 937.2565 - mae: 24.8373 - val_loss: 994.9594 - val_mae: 22.5855\n",
      "Epoch 152/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 908.7958 - mae: 24.5972 - val_loss: 996.5972 - val_mae: 22.4720\n",
      "Epoch 153/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 903.3192 - mae: 24.5308 - val_loss: 982.5833 - val_mae: 22.5154\n",
      "Epoch 154/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 899.0638 - mae: 24.5124 - val_loss: 973.2966 - val_mae: 22.4922\n",
      "Epoch 155/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 894.1408 - mae: 24.4266 - val_loss: 964.7016 - val_mae: 22.5356\n",
      "Epoch 156/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 886.8409 - mae: 24.2903 - val_loss: 994.5881 - val_mae: 22.4528\n",
      "Epoch 157/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 898.4140 - mae: 24.5607 - val_loss: 961.8569 - val_mae: 22.3427\n",
      "Epoch 158/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 883.2200 - mae: 24.1308 - val_loss: 966.0599 - val_mae: 22.5238\n",
      "Epoch 159/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 884.4108 - mae: 24.3229 - val_loss: 955.1279 - val_mae: 22.0524\n",
      "Epoch 160/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 877.5314 - mae: 24.2261 - val_loss: 969.5766 - val_mae: 22.4254\n",
      "Epoch 161/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 867.2051 - mae: 24.0221 - val_loss: 952.4677 - val_mae: 22.3163\n",
      "Epoch 162/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 870.6588 - mae: 24.1500 - val_loss: 953.6604 - val_mae: 22.2372\n",
      "Epoch 163/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 866.1207 - mae: 24.0645 - val_loss: 939.5419 - val_mae: 22.1153\n",
      "Epoch 164/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 864.5381 - mae: 24.1029 - val_loss: 941.6176 - val_mae: 22.2185\n",
      "Epoch 165/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 854.1992 - mae: 23.8851 - val_loss: 927.8599 - val_mae: 21.8891\n",
      "Epoch 166/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 851.1624 - mae: 23.8465 - val_loss: 925.9672 - val_mae: 22.1572\n",
      "Epoch 167/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 852.3201 - mae: 23.8719 - val_loss: 936.8400 - val_mae: 21.9766\n",
      "Epoch 168/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 842.3762 - mae: 23.7849 - val_loss: 914.4563 - val_mae: 21.9827\n",
      "Epoch 169/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 839.5660 - mae: 23.6963 - val_loss: 930.4514 - val_mae: 22.0006\n",
      "Epoch 170/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 841.2555 - mae: 23.6959 - val_loss: 916.8422 - val_mae: 22.0812\n",
      "Epoch 171/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 835.9744 - mae: 23.6040 - val_loss: 913.0229 - val_mae: 21.7626\n",
      "Epoch 172/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 839.1578 - mae: 23.6943 - val_loss: 900.6705 - val_mae: 21.6980\n",
      "Epoch 173/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 840.2934 - mae: 23.7120 - val_loss: 917.6967 - val_mae: 21.9982\n",
      "Epoch 174/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 833.9638 - mae: 23.5686 - val_loss: 897.9332 - val_mae: 21.9170\n",
      "Epoch 175/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 823.9916 - mae: 23.4841 - val_loss: 926.7440 - val_mae: 21.7415\n",
      "Epoch 176/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 830.5673 - mae: 23.5458 - val_loss: 911.6427 - val_mae: 21.8046\n",
      "Epoch 177/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 822.9213 - mae: 23.4580 - val_loss: 899.7119 - val_mae: 21.7231\n",
      "Epoch 178/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 824.0548 - mae: 23.4797 - val_loss: 892.8092 - val_mae: 21.5028\n",
      "Epoch 179/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 818.7704 - mae: 23.4003 - val_loss: 907.7028 - val_mae: 21.8149\n",
      "Epoch 180/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 812.9927 - mae: 23.3635 - val_loss: 903.1337 - val_mae: 21.6121\n",
      "Epoch 181/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 809.5227 - mae: 23.2858 - val_loss: 911.4974 - val_mae: 21.8009\n",
      "Epoch 182/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 816.1441 - mae: 23.3567 - val_loss: 907.2615 - val_mae: 21.6480\n",
      "Epoch 183/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 809.0429 - mae: 23.2679 - val_loss: 888.4700 - val_mae: 21.6197\n",
      "Epoch 184/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 813.1374 - mae: 23.2602 - val_loss: 889.5447 - val_mae: 21.6254\n",
      "Epoch 185/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 801.1902 - mae: 23.1503 - val_loss: 876.2005 - val_mae: 21.4703\n",
      "Epoch 186/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 804.6129 - mae: 23.2137 - val_loss: 901.5247 - val_mae: 21.5629\n",
      "Epoch 187/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 803.9588 - mae: 23.1294 - val_loss: 875.1577 - val_mae: 21.3039\n",
      "Epoch 188/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 794.9143 - mae: 22.9647 - val_loss: 885.3907 - val_mae: 21.6211\n",
      "Epoch 189/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 812.4739 - mae: 23.2178 - val_loss: 901.1302 - val_mae: 21.6303\n",
      "Epoch 190/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 799.2685 - mae: 22.9555 - val_loss: 869.3698 - val_mae: 21.3856\n",
      "Epoch 191/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 798.1087 - mae: 23.1365 - val_loss: 875.2979 - val_mae: 21.2623\n",
      "Epoch 192/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 788.2619 - mae: 22.9732 - val_loss: 870.2686 - val_mae: 21.4462\n",
      "Epoch 193/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 782.5881 - mae: 22.8455 - val_loss: 870.3305 - val_mae: 21.4146\n",
      "Epoch 194/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 781.1821 - mae: 22.8272 - val_loss: 861.1678 - val_mae: 21.3139\n",
      "Epoch 195/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 779.9655 - mae: 22.8622 - val_loss: 864.6530 - val_mae: 21.3484\n",
      "Epoch 196/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 773.9226 - mae: 22.7149 - val_loss: 878.5005 - val_mae: 21.5353\n",
      "Epoch 197/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 776.9106 - mae: 22.7204 - val_loss: 852.7606 - val_mae: 21.2689\n",
      "Epoch 198/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 773.2643 - mae: 22.7397 - val_loss: 863.1535 - val_mae: 21.3345\n",
      "Epoch 199/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 775.5753 - mae: 22.8188 - val_loss: 844.4609 - val_mae: 21.1941\n",
      "Epoch 200/200\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 769.3768 - mae: 22.6834 - val_loss: 868.0463 - val_mae: 21.2838\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=200,\n",
    "    batch_size=16,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e68df64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 27.34\n",
      "Test MSE: 1292.82\n"
     ]
    }
   ],
   "source": [
    "loss, mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test MAE: {mae:.2f}\")\n",
    "print(f\"Test MSE: {loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "555afb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)  # output layer for regression\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "925ea873",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=30,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9029138d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 4894.2373 - mae: 56.6384 - val_loss: 1353.0968 - val_mae: 30.1840\n",
      "Epoch 2/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4878.7651 - mae: 55.7457 - val_loss: 746.8669 - val_mae: 20.5832\n",
      "Epoch 3/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5341.6445 - mae: 58.5378 - val_loss: 1176.1346 - val_mae: 26.5770\n",
      "Epoch 4/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4554.4985 - mae: 53.9117 - val_loss: 1220.9138 - val_mae: 27.8620\n",
      "Epoch 5/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5607.9668 - mae: 59.9222 - val_loss: 904.8623 - val_mae: 22.3210\n",
      "Epoch 6/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5222.9233 - mae: 56.4919 - val_loss: 941.5368 - val_mae: 23.6597\n",
      "Epoch 7/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4725.7036 - mae: 53.9452 - val_loss: 1008.8417 - val_mae: 22.9581\n",
      "Epoch 8/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5711.9980 - mae: 60.3026 - val_loss: 1421.3041 - val_mae: 30.3687\n",
      "Epoch 9/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4378.9028 - mae: 51.5842 - val_loss: 840.6158 - val_mae: 21.5037\n",
      "Epoch 10/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4913.0112 - mae: 55.6009 - val_loss: 924.1669 - val_mae: 22.3050\n",
      "Epoch 11/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4722.2930 - mae: 55.2316 - val_loss: 835.8913 - val_mae: 22.1398\n",
      "Epoch 12/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4196.0083 - mae: 51.6006 - val_loss: 1151.6510 - val_mae: 27.6257\n",
      "Epoch 13/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5449.6426 - mae: 58.6766 - val_loss: 803.0383 - val_mae: 22.1198\n",
      "Epoch 14/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5013.9521 - mae: 56.4600 - val_loss: 1001.3488 - val_mae: 23.1446\n",
      "Epoch 15/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4953.4419 - mae: 56.1637 - val_loss: 1082.4048 - val_mae: 25.5344\n",
      "Epoch 16/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5241.5381 - mae: 57.3938 - val_loss: 1236.8823 - val_mae: 27.4993\n",
      "Epoch 17/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4591.7832 - mae: 54.5251 - val_loss: 963.7170 - val_mae: 24.1663\n",
      "Epoch 18/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4984.9517 - mae: 57.5227 - val_loss: 769.7031 - val_mae: 20.4448\n",
      "Epoch 19/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5193.8882 - mae: 58.4983 - val_loss: 1256.1932 - val_mae: 28.1551\n",
      "Epoch 20/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5008.1201 - mae: 56.7529 - val_loss: 1086.6281 - val_mae: 24.7846\n",
      "Epoch 21/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4641.8555 - mae: 54.6573 - val_loss: 1097.6710 - val_mae: 23.6622\n",
      "Epoch 22/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4585.5044 - mae: 54.6930 - val_loss: 1012.1437 - val_mae: 25.2994\n",
      "Epoch 23/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4219.9668 - mae: 51.7735 - val_loss: 744.7951 - val_mae: 21.1473\n",
      "Epoch 24/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5012.3447 - mae: 56.2094 - val_loss: 697.2413 - val_mae: 19.8489\n",
      "Epoch 25/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4857.0044 - mae: 55.9876 - val_loss: 737.9990 - val_mae: 20.3057\n",
      "Epoch 26/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4434.8066 - mae: 53.4172 - val_loss: 902.5642 - val_mae: 23.7399\n",
      "Epoch 27/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4700.6812 - mae: 54.6448 - val_loss: 877.9189 - val_mae: 22.0275\n",
      "Epoch 28/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4410.6260 - mae: 51.8440 - val_loss: 921.2335 - val_mae: 23.0076\n",
      "Epoch 29/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4681.2358 - mae: 55.2162 - val_loss: 1074.5010 - val_mae: 26.1460\n",
      "Epoch 30/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4739.1870 - mae: 52.9787 - val_loss: 1043.9188 - val_mae: 25.4103\n",
      "Epoch 31/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4387.4468 - mae: 51.7945 - val_loss: 906.2318 - val_mae: 23.0068\n",
      "Epoch 32/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4617.8110 - mae: 53.0440 - val_loss: 1177.8219 - val_mae: 27.5883\n",
      "Epoch 33/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4289.7358 - mae: 50.3981 - val_loss: 784.1178 - val_mae: 20.7199\n",
      "Epoch 34/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5009.2104 - mae: 56.6951 - val_loss: 1002.6068 - val_mae: 24.9059\n",
      "Epoch 35/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5124.7729 - mae: 57.0868 - val_loss: 1027.5358 - val_mae: 25.3134\n",
      "Epoch 36/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4860.5405 - mae: 54.6717 - val_loss: 935.8530 - val_mae: 22.0766\n",
      "Epoch 37/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4365.5815 - mae: 53.4852 - val_loss: 1153.4718 - val_mae: 25.1978\n",
      "Epoch 38/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4941.2310 - mae: 55.3233 - val_loss: 970.9102 - val_mae: 21.9894\n",
      "Epoch 39/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4921.8066 - mae: 55.1752 - val_loss: 1176.3772 - val_mae: 26.0638\n",
      "Epoch 40/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5099.5093 - mae: 57.2296 - val_loss: 927.7543 - val_mae: 23.2340\n",
      "Epoch 41/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4306.8335 - mae: 50.8353 - val_loss: 1081.3453 - val_mae: 26.4294\n",
      "Epoch 42/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4710.5996 - mae: 55.3896 - val_loss: 862.1432 - val_mae: 23.5135\n",
      "Epoch 43/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4601.6504 - mae: 54.6750 - val_loss: 734.1089 - val_mae: 20.4954\n",
      "Epoch 44/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4556.0264 - mae: 53.7517 - val_loss: 942.7852 - val_mae: 24.3280\n",
      "Epoch 45/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4621.4263 - mae: 53.9939 - val_loss: 1009.4422 - val_mae: 24.4501\n",
      "Epoch 46/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4869.3120 - mae: 54.0892 - val_loss: 1009.7217 - val_mae: 23.5392\n",
      "Epoch 47/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5273.7432 - mae: 57.4651 - val_loss: 927.5901 - val_mae: 23.3533\n",
      "Epoch 48/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5050.5508 - mae: 55.0973 - val_loss: 925.7620 - val_mae: 23.3587\n",
      "Epoch 49/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4401.3501 - mae: 53.1544 - val_loss: 885.8255 - val_mae: 21.5558\n",
      "Epoch 50/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4754.6069 - mae: 56.7862 - val_loss: 824.3689 - val_mae: 21.5440\n",
      "Epoch 51/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5120.2212 - mae: 56.6263 - val_loss: 857.5837 - val_mae: 21.7084\n",
      "Epoch 52/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4912.5649 - mae: 54.9272 - val_loss: 1145.3673 - val_mae: 26.6974\n",
      "Epoch 53/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4447.8198 - mae: 53.8098 - val_loss: 1392.6671 - val_mae: 30.0790\n",
      "Epoch 54/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4774.9536 - mae: 53.6360 - val_loss: 897.6437 - val_mae: 23.2200\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=300,\n",
    "    batch_size=16,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8315c291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 25.98\n",
      "Test MSE: 1169.58\n"
     ]
    }
   ],
   "source": [
    "loss, mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test MAE: {mae:.2f}\")\n",
    "print(f\"Test MSE: {loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bee9c2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with batch size: 8\n",
      "Batch size: 8 - Test MAE: 26.43, MSE: 1185.92\n",
      "\n",
      "Training model with batch size: 16\n",
      "Batch size: 16 - Test MAE: 26.64, MSE: 1142.84\n",
      "\n",
      "Training model with batch size: 32\n",
      "Batch size: 32 - Test MAE: 28.75, MSE: 1383.88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "batch_sizes = [8, 16, 32]\n",
    "best_lr = 0.001  # Set this to your best learning rate from previous step\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    print(f\"Training model with batch size: {bs}\")\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=best_lr),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=300,\n",
    "        batch_size=bs,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "    loss, mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Batch size: {bs} - Test MAE: {mae:.2f}, MSE: {loss:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca0e83f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gauta\\OneDrive\\Dokumentai\\cattle_model\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - MAE: 24.86, MSE: 1051.17\n",
      "Training fold 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gauta\\OneDrive\\Dokumentai\\cattle_model\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 - MAE: 26.91, MSE: 1049.37\n",
      "Training fold 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gauta\\OneDrive\\Dokumentai\\cattle_model\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 - MAE: 26.63, MSE: 1017.17\n",
      "Training fold 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gauta\\OneDrive\\Dokumentai\\cattle_model\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 - MAE: 24.68, MSE: 910.67\n",
      "Training fold 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gauta\\OneDrive\\Dokumentai\\cattle_model\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 - MAE: 19.54, MSE: 636.26\n",
      "\n",
      "Average MAE: 24.52 +/- 2.65\n",
      "Average MSE: 932.93 +/- 156.93\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Prepare data arrays\n",
    "X_array = X.values\n",
    "y_array = y.values\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "mae_scores = []\n",
    "mse_scores = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X_array)):\n",
    "    print(f\"Training fold {fold+1}...\")\n",
    "    X_train_fold, X_val_fold = X_array[train_index], X_array[val_index]\n",
    "    y_train_fold, y_val_fold = y_array[train_index], y_array[val_index]\n",
    "\n",
    "    # Standardize features per fold\n",
    "    scaler = StandardScaler()\n",
    "    X_train_fold = scaler.fit_transform(X_train_fold)\n",
    "    X_val_fold = scaler.transform(X_val_fold)\n",
    "\n",
    "    # Build model\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(X_train_fold.shape[1],)),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "\n",
    "    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        X_train_fold, y_train_fold,\n",
    "        validation_data=(X_val_fold, y_val_fold),\n",
    "        epochs=300,\n",
    "        batch_size=16,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    loss, mae = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
    "    print(f\"Fold {fold+1} - MAE: {mae:.2f}, MSE: {loss:.2f}\")\n",
    "    mae_scores.append(mae)\n",
    "    mse_scores.append(loss)\n",
    "\n",
    "print(f\"\\nAverage MAE: {np.mean(mae_scores):.2f} +/- {np.std(mae_scores):.2f}\")\n",
    "print(f\"Average MSE: {np.mean(mse_scores):.2f} +/- {np.std(mse_scores):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "39804a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gauta\\OneDrive\\Dokumentai\\cattle_model\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "Fold 1 - R-squared: 0.8512\n",
      "Training fold 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gauta\\OneDrive\\Dokumentai\\cattle_model\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "Fold 2 - R-squared: 0.8470\n",
      "Training fold 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gauta\\OneDrive\\Dokumentai\\cattle_model\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002414B67AFC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 36ms/stepWARNING:tensorflow:6 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002414B67AFC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Fold 3 - R-squared: 0.7569\n",
      "Training fold 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gauta\\OneDrive\\Dokumentai\\cattle_model\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "Fold 4 - R-squared: 0.8044\n",
      "Training fold 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gauta\\OneDrive\\Dokumentai\\cattle_model\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Fold 5 - R-squared: 0.9100\n",
      "\n",
      "Average R-squared: 0.8339 +/- 0.0511\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_scores = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X_array)):\n",
    "    print(f\"Training fold {fold+1}...\")\n",
    "    X_train_fold, X_val_fold = X_array[train_index], X_array[val_index]\n",
    "    y_train_fold, y_val_fold = y_array[train_index], y_array[val_index]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_fold = scaler.fit_transform(X_train_fold)\n",
    "    X_val_fold = scaler.transform(X_val_fold)\n",
    "\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(X_train_fold.shape[1],)),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "\n",
    "    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        X_train_fold, y_train_fold,\n",
    "        validation_data=(X_val_fold, y_val_fold),\n",
    "        epochs=300,\n",
    "        batch_size=16,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    predictions = model.predict(X_val_fold).flatten()\n",
    "    r2 = r2_score(y_val_fold, predictions)\n",
    "    print(f\"Fold {fold+1} - R-squared: {r2:.4f}\")\n",
    "    r2_scores.append(r2)\n",
    "\n",
    "print(f\"\\nAverage R-squared: {np.mean(r2_scores):.4f} +/- {np.std(r2_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c13ab978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost 5-Fold CV R2 scores: [0.81766719 0.78682321 0.7231366  0.72040451 0.86223674]\n",
      "Average R2: 0.7821 ± 0.0548\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import make_scorer, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Prepare data\n",
    "X_array = X.values\n",
    "y_array = y.values\n",
    "\n",
    "# Define XGBoost regressor\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# Custom scorer for R2\n",
    "r2_scorer = make_scorer(r2_score)\n",
    "\n",
    "# 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "r2_scores = cross_val_score(xgb_model, X_array, y_array, scoring=r2_scorer, cv=kf)\n",
    "\n",
    "print(f\"XGBoost 5-Fold CV R2 scores: {r2_scores}\")\n",
    "print(f\"Average R2: {np.mean(r2_scores):.4f} ± {np.std(r2_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9106ea90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest 5-Fold CV R2 scores: [0.84254042 0.84382278 0.75759671 0.77874787 0.88462074]\n",
      "Average R2: 0.8215 ± 0.0466\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Prepare data\n",
    "X_array = X.values\n",
    "y_array = y.values\n",
    "\n",
    "# Define Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(random_state=42, n_estimators=100)\n",
    "\n",
    "# Custom scorer for R2\n",
    "r2_scorer = make_scorer(r2_score)\n",
    "\n",
    "# 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "r2_scores = cross_val_score(rf_model, X_array, y_array, scoring=r2_scorer, cv=kf)\n",
    "\n",
    "print(f\"Random Forest 5-Fold CV R2 scores: {r2_scores}\")\n",
    "print(f\"Average R2: {np.mean(r2_scores):.4f} ± {np.std(r2_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a36e0315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features: 10\n",
      "Polynomial features after transformation: 65\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Check new shape\n",
    "print(f\"Original features: {X.shape[1]}\")\n",
    "print(f\"Polynomial features after transformation: {X_poly.shape[1]}\")\n",
    "\n",
    "# Proceed with splitting, scaling, and training as before, but using X_poly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ff5dde3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gauta\\OneDrive\\Dokumentai\\cattle_model\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 365005.7188 - mae: 599.5800 - val_loss: 374959.0938 - val_mae: 607.2308\n",
      "Epoch 2/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 352152.3125 - mae: 588.7757 - val_loss: 344078.3125 - val_mae: 582.2297\n",
      "Epoch 3/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 297762.4688 - mae: 538.7284 - val_loss: 231608.8438 - val_mae: 471.7243\n",
      "Epoch 4/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 174296.9219 - mae: 384.4925 - val_loss: 120042.5781 - val_mae: 303.3246\n",
      "Epoch 5/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 127020.9453 - mae: 311.5905 - val_loss: 110711.3984 - val_mae: 292.9860\n",
      "Epoch 6/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 115045.2344 - mae: 294.9956 - val_loss: 102951.1328 - val_mae: 285.0571\n",
      "Epoch 7/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 102262.7656 - mae: 279.4567 - val_loss: 92533.3828 - val_mae: 272.2722\n",
      "Epoch 8/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 90895.1406 - mae: 258.7139 - val_loss: 81918.0391 - val_mae: 257.3786\n",
      "Epoch 9/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 78425.7656 - mae: 242.8034 - val_loss: 69839.4219 - val_mae: 237.0056\n",
      "Epoch 10/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 65453.5273 - mae: 219.3177 - val_loss: 59390.4805 - val_mae: 216.9325\n",
      "Epoch 11/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 56947.5078 - mae: 204.4420 - val_loss: 49738.2227 - val_mae: 195.6967\n",
      "Epoch 12/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 45648.7617 - mae: 179.2811 - val_loss: 40944.9062 - val_mae: 175.1608\n",
      "Epoch 13/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 38294.4727 - mae: 161.3031 - val_loss: 35593.7188 - val_mae: 162.2703\n",
      "Epoch 14/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 33566.1562 - mae: 153.4059 - val_loss: 30640.2930 - val_mae: 149.0195\n",
      "Epoch 15/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 31304.7188 - mae: 145.4377 - val_loss: 26828.1348 - val_mae: 136.9914\n",
      "Epoch 16/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 25349.1465 - mae: 131.1397 - val_loss: 24837.2656 - val_mae: 129.4932\n",
      "Epoch 17/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 28033.4629 - mae: 136.0654 - val_loss: 20495.5156 - val_mae: 118.2901\n",
      "Epoch 18/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 27577.3301 - mae: 134.7925 - val_loss: 20032.8184 - val_mae: 113.0788\n",
      "Epoch 19/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 22418.5879 - mae: 122.4672 - val_loss: 20032.8594 - val_mae: 112.1602\n",
      "Epoch 20/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 20545.6562 - mae: 115.7014 - val_loss: 17238.1426 - val_mae: 104.0845\n",
      "Epoch 21/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 19922.5977 - mae: 113.9643 - val_loss: 17246.8145 - val_mae: 103.5280\n",
      "Epoch 22/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 20922.8203 - mae: 119.8028 - val_loss: 16503.4102 - val_mae: 103.4917\n",
      "Epoch 23/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 17913.0977 - mae: 109.0622 - val_loss: 14811.4502 - val_mae: 95.5468\n",
      "Epoch 24/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 20609.9043 - mae: 113.6053 - val_loss: 14787.0449 - val_mae: 96.4490\n",
      "Epoch 25/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 18086.0312 - mae: 110.5526 - val_loss: 15165.9492 - val_mae: 95.0379\n",
      "Epoch 26/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 19012.3691 - mae: 110.5732 - val_loss: 15620.7227 - val_mae: 98.6457\n",
      "Epoch 27/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15449.2666 - mae: 100.7075 - val_loss: 13627.0771 - val_mae: 89.2661\n",
      "Epoch 28/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 17767.7520 - mae: 108.6089 - val_loss: 14317.3838 - val_mae: 93.2359\n",
      "Epoch 29/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 16954.0918 - mae: 103.4840 - val_loss: 12614.4131 - val_mae: 88.4297\n",
      "Epoch 30/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15436.5107 - mae: 101.6148 - val_loss: 12455.8369 - val_mae: 87.7015\n",
      "Epoch 31/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15148.3350 - mae: 99.1256 - val_loss: 11280.5703 - val_mae: 83.9905\n",
      "Epoch 32/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15596.6641 - mae: 101.8514 - val_loss: 12240.0703 - val_mae: 84.8449\n",
      "Epoch 33/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 15920.2471 - mae: 101.5216 - val_loss: 11172.7227 - val_mae: 83.8148\n",
      "Epoch 34/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14678.6016 - mae: 98.7848 - val_loss: 10951.4443 - val_mae: 81.6851\n",
      "Epoch 35/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14378.3740 - mae: 95.7911 - val_loss: 9504.5693 - val_mae: 77.1993\n",
      "Epoch 36/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13870.6738 - mae: 95.1748 - val_loss: 10298.2773 - val_mae: 79.3639\n",
      "Epoch 37/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12339.1699 - mae: 92.3696 - val_loss: 9993.0605 - val_mae: 77.1087\n",
      "Epoch 38/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14949.0605 - mae: 98.9223 - val_loss: 9388.4639 - val_mae: 76.3417\n",
      "Epoch 39/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15731.6016 - mae: 100.9374 - val_loss: 9359.8770 - val_mae: 76.2107\n",
      "Epoch 40/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11135.7217 - mae: 87.3957 - val_loss: 11208.9619 - val_mae: 79.6024\n",
      "Epoch 41/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12695.4209 - mae: 92.0853 - val_loss: 10335.3145 - val_mae: 77.8813\n",
      "Epoch 42/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13753.5166 - mae: 92.1869 - val_loss: 8606.0615 - val_mae: 73.7813\n",
      "Epoch 43/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 12560.5918 - mae: 92.4486 - val_loss: 9617.6602 - val_mae: 74.4621\n",
      "Epoch 44/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13837.3398 - mae: 95.2234 - val_loss: 9489.7500 - val_mae: 74.4106\n",
      "Epoch 45/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13518.3213 - mae: 93.5081 - val_loss: 9245.6895 - val_mae: 74.0395\n",
      "Epoch 46/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11274.2080 - mae: 86.7046 - val_loss: 8662.9600 - val_mae: 70.9210\n",
      "Epoch 47/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12412.5312 - mae: 88.8938 - val_loss: 9870.0957 - val_mae: 75.9101\n",
      "Epoch 48/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9201.5596 - mae: 75.9624 - val_loss: 7382.9111 - val_mae: 66.8882\n",
      "Epoch 49/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11164.5449 - mae: 85.7665 - val_loss: 8054.7393 - val_mae: 69.6768\n",
      "Epoch 50/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 10929.6562 - mae: 84.7399 - val_loss: 6723.0039 - val_mae: 65.7948\n",
      "Epoch 51/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12358.5664 - mae: 89.6683 - val_loss: 7965.2183 - val_mae: 69.2628\n",
      "Epoch 52/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11695.3262 - mae: 89.9320 - val_loss: 6160.4746 - val_mae: 62.0296\n",
      "Epoch 53/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 10508.0391 - mae: 84.7627 - val_loss: 8348.1064 - val_mae: 72.2861\n",
      "Epoch 54/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9782.8945 - mae: 79.6992 - val_loss: 8421.9473 - val_mae: 71.0001\n",
      "Epoch 55/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11422.7686 - mae: 84.9955 - val_loss: 7000.8931 - val_mae: 65.2657\n",
      "Epoch 56/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10742.3242 - mae: 84.2906 - val_loss: 7802.1196 - val_mae: 69.0082\n",
      "Epoch 57/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8572.9893 - mae: 75.9480 - val_loss: 6439.2852 - val_mae: 62.1538\n",
      "Epoch 58/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9274.8252 - mae: 76.4053 - val_loss: 6670.1030 - val_mae: 62.2968\n",
      "Epoch 59/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8732.4131 - mae: 73.8446 - val_loss: 7616.3940 - val_mae: 65.5098\n",
      "Epoch 60/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8929.4570 - mae: 75.4376 - val_loss: 5855.2500 - val_mae: 60.1110\n",
      "Epoch 61/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7875.0908 - mae: 72.4523 - val_loss: 5207.2695 - val_mae: 55.7639\n",
      "Epoch 62/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8447.8359 - mae: 71.7209 - val_loss: 5379.9336 - val_mae: 58.2163\n",
      "Epoch 63/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8600.7383 - mae: 75.1520 - val_loss: 4841.5171 - val_mae: 54.4282\n",
      "Epoch 64/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9592.2725 - mae: 77.1101 - val_loss: 4400.7607 - val_mae: 52.2209\n",
      "Epoch 65/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7662.7583 - mae: 71.2663 - val_loss: 4749.5024 - val_mae: 52.8878\n",
      "Epoch 66/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9988.9697 - mae: 81.3430 - val_loss: 3957.8713 - val_mae: 51.2407\n",
      "Epoch 67/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7758.0728 - mae: 72.4463 - val_loss: 4286.7339 - val_mae: 50.8993\n",
      "Epoch 68/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8033.5244 - mae: 70.4053 - val_loss: 4867.6084 - val_mae: 55.2281\n",
      "Epoch 69/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8105.0425 - mae: 71.8811 - val_loss: 3969.1814 - val_mae: 49.2525\n",
      "Epoch 70/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8653.8330 - mae: 75.6926 - val_loss: 4168.4551 - val_mae: 51.0031\n",
      "Epoch 71/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9148.1162 - mae: 75.8266 - val_loss: 3818.2612 - val_mae: 47.3870\n",
      "Epoch 72/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7659.8931 - mae: 70.7261 - val_loss: 3640.0112 - val_mae: 49.5436\n",
      "Epoch 73/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8542.5322 - mae: 72.9610 - val_loss: 4581.0127 - val_mae: 52.7425\n",
      "Epoch 74/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7407.3105 - mae: 68.4490 - val_loss: 3191.0308 - val_mae: 43.7261\n",
      "Epoch 75/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7167.6250 - mae: 66.4521 - val_loss: 3877.7273 - val_mae: 48.0644\n",
      "Epoch 76/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7573.9268 - mae: 70.2392 - val_loss: 3660.7703 - val_mae: 46.6432\n",
      "Epoch 77/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7342.1411 - mae: 67.9036 - val_loss: 3281.6672 - val_mae: 45.3841\n",
      "Epoch 78/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7377.6611 - mae: 69.6090 - val_loss: 3458.9534 - val_mae: 45.9011\n",
      "Epoch 79/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6853.4639 - mae: 64.6338 - val_loss: 3986.6792 - val_mae: 48.7700\n",
      "Epoch 80/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7365.1733 - mae: 68.3188 - val_loss: 3454.4702 - val_mae: 44.7226\n",
      "Epoch 81/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7535.5537 - mae: 68.8241 - val_loss: 2985.9446 - val_mae: 41.9978\n",
      "Epoch 82/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7458.6099 - mae: 68.8043 - val_loss: 3135.3755 - val_mae: 45.2046\n",
      "Epoch 83/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7039.6577 - mae: 65.8080 - val_loss: 2809.2776 - val_mae: 41.4551\n",
      "Epoch 84/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6604.3394 - mae: 64.8815 - val_loss: 2950.7661 - val_mae: 42.5668\n",
      "Epoch 85/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6526.5220 - mae: 64.4888 - val_loss: 2732.4470 - val_mae: 40.7042\n",
      "Epoch 86/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8421.2988 - mae: 74.7597 - val_loss: 2649.2578 - val_mae: 40.2914\n",
      "Epoch 87/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6834.7666 - mae: 65.7613 - val_loss: 2890.7415 - val_mae: 40.2538\n",
      "Epoch 88/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7526.3726 - mae: 71.5219 - val_loss: 2568.3376 - val_mae: 39.4971\n",
      "Epoch 89/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6242.8442 - mae: 62.5113 - val_loss: 2596.5051 - val_mae: 39.0582\n",
      "Epoch 90/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6551.2910 - mae: 64.1202 - val_loss: 2521.3525 - val_mae: 39.1185\n",
      "Epoch 91/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7431.7144 - mae: 69.0864 - val_loss: 2911.0115 - val_mae: 41.0298\n",
      "Epoch 92/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7082.4902 - mae: 67.5404 - val_loss: 2470.2356 - val_mae: 37.6517\n",
      "Epoch 93/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6159.3750 - mae: 62.6078 - val_loss: 2752.2339 - val_mae: 39.3186\n",
      "Epoch 94/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5830.7236 - mae: 62.2917 - val_loss: 3168.7180 - val_mae: 41.6631\n",
      "Epoch 95/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6050.3433 - mae: 61.4100 - val_loss: 2703.0300 - val_mae: 39.3661\n",
      "Epoch 96/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5775.4038 - mae: 59.6513 - val_loss: 2191.7915 - val_mae: 36.8402\n",
      "Epoch 97/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6031.9961 - mae: 62.5191 - val_loss: 2690.1904 - val_mae: 38.9422\n",
      "Epoch 98/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6594.0151 - mae: 64.0536 - val_loss: 3248.6868 - val_mae: 43.6793\n",
      "Epoch 99/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7848.2222 - mae: 69.3364 - val_loss: 2873.0168 - val_mae: 40.7988\n",
      "Epoch 100/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6731.4893 - mae: 65.7730 - val_loss: 2198.3416 - val_mae: 38.7417\n",
      "Epoch 101/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6367.7529 - mae: 61.7487 - val_loss: 1797.4100 - val_mae: 32.4334\n",
      "Epoch 102/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5698.0679 - mae: 61.0079 - val_loss: 2745.6672 - val_mae: 40.4972\n",
      "Epoch 103/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5848.1855 - mae: 61.7358 - val_loss: 2023.1555 - val_mae: 34.2388\n",
      "Epoch 104/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6060.9316 - mae: 61.8811 - val_loss: 1779.3563 - val_mae: 34.2896\n",
      "Epoch 105/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5602.8081 - mae: 58.1769 - val_loss: 1823.7152 - val_mae: 33.0407\n",
      "Epoch 106/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6580.8506 - mae: 63.9118 - val_loss: 1661.5343 - val_mae: 31.4264\n",
      "Epoch 107/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7378.6650 - mae: 68.4099 - val_loss: 1994.9846 - val_mae: 34.6741\n",
      "Epoch 108/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6165.7544 - mae: 60.5119 - val_loss: 2337.5688 - val_mae: 36.6676\n",
      "Epoch 109/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7100.4507 - mae: 67.3625 - val_loss: 1588.9215 - val_mae: 31.4808\n",
      "Epoch 110/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5433.0527 - mae: 57.6599 - val_loss: 1423.7515 - val_mae: 30.1187\n",
      "Epoch 111/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6305.3291 - mae: 63.4451 - val_loss: 1827.8994 - val_mae: 32.6556\n",
      "Epoch 112/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5317.7905 - mae: 58.3419 - val_loss: 1764.1514 - val_mae: 32.7688\n",
      "Epoch 113/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5963.4209 - mae: 61.4918 - val_loss: 1442.8052 - val_mae: 29.2615\n",
      "Epoch 114/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5322.2197 - mae: 57.8748 - val_loss: 1619.7084 - val_mae: 29.8900\n",
      "Epoch 115/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5762.6074 - mae: 59.9578 - val_loss: 1607.3333 - val_mae: 31.2152\n",
      "Epoch 116/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6677.6660 - mae: 65.6735 - val_loss: 1453.1670 - val_mae: 28.5187\n",
      "Epoch 117/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5533.0674 - mae: 59.7625 - val_loss: 1846.7916 - val_mae: 33.6230\n",
      "Epoch 118/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6117.8018 - mae: 61.0259 - val_loss: 2304.7742 - val_mae: 35.7773\n",
      "Epoch 119/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6505.8970 - mae: 65.1793 - val_loss: 1814.0139 - val_mae: 33.0011\n",
      "Epoch 120/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6171.5210 - mae: 62.8282 - val_loss: 1818.8865 - val_mae: 31.8500\n",
      "Epoch 121/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6097.0649 - mae: 63.1938 - val_loss: 2063.6167 - val_mae: 33.6810\n",
      "Epoch 122/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5904.3447 - mae: 61.0162 - val_loss: 2768.7776 - val_mae: 42.6204\n",
      "Epoch 123/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7134.5083 - mae: 67.5370 - val_loss: 1506.5861 - val_mae: 31.6925\n",
      "Epoch 124/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5948.2705 - mae: 60.4773 - val_loss: 1412.8584 - val_mae: 29.1129\n",
      "Epoch 125/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5726.8237 - mae: 59.0243 - val_loss: 1539.6472 - val_mae: 29.3831\n",
      "Epoch 126/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5441.5381 - mae: 58.1387 - val_loss: 1493.6787 - val_mae: 28.2084\n",
      "Epoch 127/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5715.1216 - mae: 60.0538 - val_loss: 1172.0521 - val_mae: 26.6750\n",
      "Epoch 128/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7192.5112 - mae: 68.1567 - val_loss: 1564.8342 - val_mae: 29.9188\n",
      "Epoch 129/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6027.9634 - mae: 63.1731 - val_loss: 1642.3635 - val_mae: 31.6160\n",
      "Epoch 130/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6293.2866 - mae: 65.2229 - val_loss: 1298.2346 - val_mae: 27.6197\n",
      "Epoch 131/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5532.3535 - mae: 59.5796 - val_loss: 1140.0754 - val_mae: 27.1811\n",
      "Epoch 132/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6052.4985 - mae: 61.2329 - val_loss: 1373.2716 - val_mae: 29.3805\n",
      "Epoch 133/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6359.3008 - mae: 63.6833 - val_loss: 1376.5253 - val_mae: 30.8668\n",
      "Epoch 134/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5976.8081 - mae: 62.0949 - val_loss: 1513.7872 - val_mae: 28.6645\n",
      "Epoch 135/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5995.7007 - mae: 61.6128 - val_loss: 1446.4626 - val_mae: 30.0662\n",
      "Epoch 136/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5979.3892 - mae: 61.8129 - val_loss: 1080.3704 - val_mae: 26.8305\n",
      "Epoch 137/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6215.9077 - mae: 63.4886 - val_loss: 1396.5521 - val_mae: 30.4443\n",
      "Epoch 138/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5772.7383 - mae: 60.4649 - val_loss: 1866.8209 - val_mae: 35.2262\n",
      "Epoch 139/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5588.8501 - mae: 59.5382 - val_loss: 1086.9285 - val_mae: 25.5145\n",
      "Epoch 140/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4992.1733 - mae: 57.2568 - val_loss: 1198.6212 - val_mae: 26.1197\n",
      "Epoch 141/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5740.0415 - mae: 60.5870 - val_loss: 1838.4673 - val_mae: 32.6471\n",
      "Epoch 142/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5589.8071 - mae: 58.5296 - val_loss: 1437.5420 - val_mae: 28.2924\n",
      "Epoch 143/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5334.9023 - mae: 55.9012 - val_loss: 1051.2639 - val_mae: 25.0101\n",
      "Epoch 144/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5376.6748 - mae: 60.0834 - val_loss: 1429.4460 - val_mae: 29.9246\n",
      "Epoch 145/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5926.5103 - mae: 59.8901 - val_loss: 1531.8633 - val_mae: 32.1078\n",
      "Epoch 146/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5157.8286 - mae: 58.3582 - val_loss: 1325.2128 - val_mae: 27.9652\n",
      "Epoch 147/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4911.5029 - mae: 56.2065 - val_loss: 1588.1256 - val_mae: 31.6563\n",
      "Epoch 148/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5239.3848 - mae: 57.7398 - val_loss: 1200.3551 - val_mae: 28.4150\n",
      "Epoch 149/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6547.8369 - mae: 64.9248 - val_loss: 2196.1836 - val_mae: 39.1562\n",
      "Epoch 150/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5734.1831 - mae: 59.8369 - val_loss: 1111.1394 - val_mae: 24.9210\n",
      "Epoch 151/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5375.6152 - mae: 60.2159 - val_loss: 1620.2316 - val_mae: 29.5401\n",
      "Epoch 152/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5876.0093 - mae: 61.9376 - val_loss: 2062.2148 - val_mae: 32.6271\n",
      "Epoch 153/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5587.0225 - mae: 59.6871 - val_loss: 1488.5911 - val_mae: 26.9791\n",
      "Epoch 154/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5899.4526 - mae: 62.0532 - val_loss: 1656.9196 - val_mae: 32.4018\n",
      "Epoch 155/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5049.1177 - mae: 56.0233 - val_loss: 899.2509 - val_mae: 23.2653\n",
      "Epoch 156/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5205.5796 - mae: 57.3548 - val_loss: 1295.2094 - val_mae: 28.6660\n",
      "Epoch 157/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6044.4448 - mae: 63.3323 - val_loss: 1229.4115 - val_mae: 27.7339\n",
      "Epoch 158/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4993.7017 - mae: 55.1697 - val_loss: 1194.5530 - val_mae: 24.2262\n",
      "Epoch 159/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5128.7178 - mae: 58.0081 - val_loss: 1011.7963 - val_mae: 24.1793\n",
      "Epoch 160/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4849.0078 - mae: 55.7217 - val_loss: 1012.2385 - val_mae: 24.1378\n",
      "Epoch 161/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4880.2725 - mae: 54.6566 - val_loss: 1643.4628 - val_mae: 31.9427\n",
      "Epoch 162/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5817.9824 - mae: 58.8624 - val_loss: 1433.0295 - val_mae: 28.2120\n",
      "Epoch 163/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6165.5581 - mae: 62.0535 - val_loss: 1405.6499 - val_mae: 29.2051\n",
      "Epoch 164/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6247.3042 - mae: 64.3923 - val_loss: 1142.7396 - val_mae: 26.3673\n",
      "Epoch 165/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5215.0562 - mae: 56.3782 - val_loss: 1218.4756 - val_mae: 25.9419\n",
      "Epoch 166/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5949.1724 - mae: 61.5255 - val_loss: 1761.3605 - val_mae: 31.9159\n",
      "Epoch 167/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6213.1001 - mae: 62.3544 - val_loss: 2705.7139 - val_mae: 41.3052\n",
      "Epoch 168/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5625.3833 - mae: 59.7368 - val_loss: 1570.1044 - val_mae: 30.7254\n",
      "Epoch 169/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5525.7153 - mae: 60.0274 - val_loss: 1083.7559 - val_mae: 26.8469\n",
      "Epoch 170/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5109.6523 - mae: 56.9552 - val_loss: 1048.9058 - val_mae: 24.7125\n",
      "Epoch 171/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5433.5625 - mae: 58.4135 - val_loss: 1427.4059 - val_mae: 28.4078\n",
      "Epoch 172/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5303.8188 - mae: 59.7073 - val_loss: 1411.7615 - val_mae: 29.9497\n",
      "Epoch 173/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4385.9082 - mae: 53.3823 - val_loss: 989.3159 - val_mae: 24.5239\n",
      "Epoch 174/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5615.0928 - mae: 61.5463 - val_loss: 1794.9791 - val_mae: 33.5257\n",
      "Epoch 175/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4899.5327 - mae: 55.8463 - val_loss: 917.5356 - val_mae: 22.3305\n",
      "Epoch 176/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5090.9043 - mae: 58.0151 - val_loss: 947.9502 - val_mae: 23.6123\n",
      "Epoch 177/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5454.2676 - mae: 59.0303 - val_loss: 952.3611 - val_mae: 22.8165\n",
      "Epoch 178/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5528.6489 - mae: 60.4856 - val_loss: 989.9843 - val_mae: 23.5523\n",
      "Epoch 179/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5494.1162 - mae: 59.2196 - val_loss: 1050.6329 - val_mae: 25.5021\n",
      "Epoch 180/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5502.5947 - mae: 58.3609 - val_loss: 1460.0652 - val_mae: 31.5856\n",
      "Epoch 181/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5117.8706 - mae: 56.9929 - val_loss: 1165.1218 - val_mae: 25.6850\n",
      "Epoch 182/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5483.3867 - mae: 57.9509 - val_loss: 1822.3510 - val_mae: 34.4746\n",
      "Epoch 183/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5496.4233 - mae: 59.0257 - val_loss: 1004.4764 - val_mae: 24.3599\n",
      "Epoch 184/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4981.4297 - mae: 56.4455 - val_loss: 1366.2798 - val_mae: 28.7404\n",
      "Epoch 185/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4255.9858 - mae: 52.5108 - val_loss: 1011.5369 - val_mae: 24.8713\n",
      "Test MAE: 30.18\n",
      "Test MSE: 1494.14\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "Test R-squared: 0.7974\n"
     ]
    }
   ],
   "source": [
    "# Use X_poly for features and y as target\n",
    "X = pd.DataFrame(X_poly)  # Convert to DataFrame if needed\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build model (same architecture as before or feel free to tweak)\n",
    "model = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='mse', \n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=30, \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=300,\n",
    "    batch_size=16,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "loss, mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test MAE: {mae:.2f}\")\n",
    "print(f\"Test MSE: {loss:.2f}\")\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test).flatten()\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Test R-squared: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f22d8585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original feature count: 65\n",
      "Selected feature count: (517, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Fit Random Forest on polynomial features\n",
    "rf_selector = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_selector.fit(X, y)\n",
    "\n",
    "# Select features with importance above the mean importance\n",
    "selector = SelectFromModel(rf_selector, threshold='mean', prefit=True)\n",
    "X_selected = selector.transform(X)\n",
    "\n",
    "print(f\"Original feature count: {X.shape[1]}\")\n",
    "print(f\"Selected feature count: {X_selected.shape}\")\n",
    "\n",
    "# Now X_selected has the reduced set of important features\n",
    "# Proceed with train-test split, scaling, and neural network training using X_selected\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8771d946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gauta\\OneDrive\\Dokumentai\\cattle_model\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 366687.3750 - mae: 600.9410 - val_loss: 380386.0625 - val_mae: 611.4863\n",
      "Epoch 2/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 365590.5625 - mae: 600.0330 - val_loss: 378797.4062 - val_mae: 610.2303\n",
      "Epoch 3/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 363753.5625 - mae: 598.5067 - val_loss: 375947.7812 - val_mae: 607.9678\n",
      "Epoch 4/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 360592.4375 - mae: 595.8808 - val_loss: 370887.0938 - val_mae: 603.9376\n",
      "Epoch 5/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 354855.2812 - mae: 591.0599 - val_loss: 362799.2812 - val_mae: 597.4111\n",
      "Epoch 6/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 346322.1875 - mae: 583.7698 - val_loss: 350877.2188 - val_mae: 587.5955\n",
      "Epoch 7/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 334205.8438 - mae: 573.2090 - val_loss: 333776.9062 - val_mae: 573.1552\n",
      "Epoch 8/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 316914.9688 - mae: 557.8077 - val_loss: 311377.1562 - val_mae: 553.4183\n",
      "Epoch 9/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 294348.8438 - mae: 536.6207 - val_loss: 283095.4688 - val_mae: 527.1685\n",
      "Epoch 10/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 267513.4062 - mae: 510.0262 - val_loss: 249883.3281 - val_mae: 493.8109\n",
      "Epoch 11/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 236191.2188 - mae: 476.6078 - val_loss: 212511.2031 - val_mae: 452.0705\n",
      "Epoch 12/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 202111.1406 - mae: 436.1662 - val_loss: 175396.7344 - val_mae: 404.3225\n",
      "Epoch 13/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 170600.4531 - mae: 394.6273 - val_loss: 141948.5938 - val_mae: 353.9349\n",
      "Epoch 14/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 139754.1094 - mae: 350.4978 - val_loss: 114825.7734 - val_mae: 307.8365\n",
      "Epoch 15/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 114938.2031 - mae: 309.9732 - val_loss: 96073.4844 - val_mae: 271.4026\n",
      "Epoch 16/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 100616.4453 - mae: 287.3682 - val_loss: 85591.0391 - val_mae: 255.0396\n",
      "Epoch 17/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 87469.6094 - mae: 266.2049 - val_loss: 80189.6016 - val_mae: 248.2741\n",
      "Epoch 18/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 80006.3203 - mae: 252.5417 - val_loss: 77855.5391 - val_mae: 246.0722\n",
      "Epoch 19/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 76205.1797 - mae: 244.7036 - val_loss: 75707.5625 - val_mae: 242.6470\n",
      "Epoch 20/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 68493.5078 - mae: 233.5695 - val_loss: 73367.1641 - val_mae: 238.4150\n",
      "Epoch 21/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 67522.0938 - mae: 231.2093 - val_loss: 70709.8984 - val_mae: 233.6744\n",
      "Epoch 22/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 65606.3906 - mae: 223.3518 - val_loss: 67940.2812 - val_mae: 228.6205\n",
      "Epoch 23/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 63430.1406 - mae: 218.5820 - val_loss: 62916.3477 - val_mae: 220.7909\n",
      "Epoch 24/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 58717.8906 - mae: 213.5835 - val_loss: 59200.7969 - val_mae: 214.0123\n",
      "Epoch 25/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 55354.3750 - mae: 206.3981 - val_loss: 55883.2891 - val_mae: 207.3690\n",
      "Epoch 26/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 49217.0078 - mae: 197.6093 - val_loss: 52146.3789 - val_mae: 200.0628\n",
      "Epoch 27/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 46737.0352 - mae: 187.8838 - val_loss: 48460.1836 - val_mae: 192.1422\n",
      "Epoch 28/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 43342.0703 - mae: 177.7156 - val_loss: 45024.9609 - val_mae: 184.5430\n",
      "Epoch 29/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 40454.9414 - mae: 177.0953 - val_loss: 40313.0820 - val_mae: 174.9890\n",
      "Epoch 30/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 35937.3516 - mae: 163.8608 - val_loss: 37024.6523 - val_mae: 166.7204\n",
      "Epoch 31/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 32396.6152 - mae: 153.4999 - val_loss: 33386.4609 - val_mae: 157.6797\n",
      "Epoch 32/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 31009.0098 - mae: 149.6570 - val_loss: 29241.5332 - val_mae: 147.5398\n",
      "Epoch 33/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 29854.6270 - mae: 142.1808 - val_loss: 25939.4551 - val_mae: 138.1931\n",
      "Epoch 34/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 21937.4023 - mae: 124.1135 - val_loss: 23260.0195 - val_mae: 129.6919\n",
      "Epoch 35/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 21693.5469 - mae: 121.8291 - val_loss: 20469.8672 - val_mae: 120.9716\n",
      "Epoch 36/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 19117.3965 - mae: 114.2456 - val_loss: 17460.0781 - val_mae: 111.2424\n",
      "Epoch 37/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 17824.0234 - mae: 105.4256 - val_loss: 15415.6113 - val_mae: 104.0622\n",
      "Epoch 38/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14005.1650 - mae: 96.2559 - val_loss: 13312.6201 - val_mae: 96.3950\n",
      "Epoch 39/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12730.6729 - mae: 90.9176 - val_loss: 11692.4395 - val_mae: 89.4667\n",
      "Epoch 40/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13961.8740 - mae: 91.7333 - val_loss: 10169.2920 - val_mae: 82.6189\n",
      "Epoch 41/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10496.9189 - mae: 80.0542 - val_loss: 8911.5322 - val_mae: 77.3985\n",
      "Epoch 42/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11397.3809 - mae: 80.7453 - val_loss: 7728.2773 - val_mae: 72.6054\n",
      "Epoch 43/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9641.0635 - mae: 78.1875 - val_loss: 6620.5474 - val_mae: 66.5349\n",
      "Epoch 44/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9396.4150 - mae: 75.1366 - val_loss: 5775.7344 - val_mae: 61.5600\n",
      "Epoch 45/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9962.8486 - mae: 75.5174 - val_loss: 4941.6372 - val_mae: 56.7279\n",
      "Epoch 46/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8194.4189 - mae: 68.2682 - val_loss: 4506.0078 - val_mae: 54.7080\n",
      "Epoch 47/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7949.1768 - mae: 65.2844 - val_loss: 4077.9849 - val_mae: 52.2126\n",
      "Epoch 48/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7550.3022 - mae: 66.3593 - val_loss: 3844.6023 - val_mae: 51.4249\n",
      "Epoch 49/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7165.6978 - mae: 66.5299 - val_loss: 3205.7891 - val_mae: 46.7573\n",
      "Epoch 50/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6169.9536 - mae: 59.7196 - val_loss: 2861.9421 - val_mae: 44.2453\n",
      "Epoch 51/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6146.0977 - mae: 61.0756 - val_loss: 2693.0571 - val_mae: 42.8879\n",
      "Epoch 52/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6158.4453 - mae: 59.1092 - val_loss: 2349.7412 - val_mae: 40.0915\n",
      "Epoch 53/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5439.1748 - mae: 56.8549 - val_loss: 2171.1846 - val_mae: 38.5546\n",
      "Epoch 54/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5702.7734 - mae: 57.9329 - val_loss: 1897.0874 - val_mae: 35.6183\n",
      "Epoch 55/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5543.5396 - mae: 56.9451 - val_loss: 1745.6361 - val_mae: 34.1263\n",
      "Epoch 56/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5128.0464 - mae: 55.5745 - val_loss: 1679.0681 - val_mae: 33.9628\n",
      "Epoch 57/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4932.8677 - mae: 55.8358 - val_loss: 1623.8784 - val_mae: 33.8417\n",
      "Epoch 58/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4922.7715 - mae: 55.3540 - val_loss: 1444.9104 - val_mae: 31.5907\n",
      "Epoch 59/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4673.0361 - mae: 52.7146 - val_loss: 1314.9897 - val_mae: 29.8731\n",
      "Epoch 60/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4309.9126 - mae: 51.4609 - val_loss: 1295.6774 - val_mae: 29.9035\n",
      "Epoch 61/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4314.4937 - mae: 51.3689 - val_loss: 1244.2733 - val_mae: 29.2443\n",
      "Epoch 62/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4231.3315 - mae: 51.0589 - val_loss: 1243.6576 - val_mae: 29.0146\n",
      "Epoch 63/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5034.8516 - mae: 56.3857 - val_loss: 1268.7576 - val_mae: 29.9348\n",
      "Epoch 64/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4208.8369 - mae: 51.1108 - val_loss: 1150.3600 - val_mae: 28.2790\n",
      "Epoch 65/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4519.5825 - mae: 52.9521 - val_loss: 1024.8702 - val_mae: 25.8146\n",
      "Epoch 66/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4085.8223 - mae: 51.3822 - val_loss: 992.6672 - val_mae: 25.5047\n",
      "Epoch 67/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4083.4312 - mae: 49.2947 - val_loss: 1060.3939 - val_mae: 27.1552\n",
      "Epoch 68/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4246.0532 - mae: 50.6271 - val_loss: 975.1597 - val_mae: 25.2841\n",
      "Epoch 69/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4125.3452 - mae: 50.3683 - val_loss: 937.1238 - val_mae: 24.6396\n",
      "Epoch 70/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3988.5400 - mae: 48.7871 - val_loss: 907.6372 - val_mae: 24.1800\n",
      "Epoch 71/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3903.6807 - mae: 50.2860 - val_loss: 891.5604 - val_mae: 23.7252\n",
      "Epoch 72/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4147.4160 - mae: 49.8781 - val_loss: 872.2935 - val_mae: 23.6473\n",
      "Epoch 73/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4043.4863 - mae: 50.9778 - val_loss: 910.6753 - val_mae: 24.2700\n",
      "Epoch 74/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3998.5791 - mae: 48.7574 - val_loss: 846.9327 - val_mae: 23.2170\n",
      "Epoch 75/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4180.8945 - mae: 52.2954 - val_loss: 807.5595 - val_mae: 22.4516\n",
      "Epoch 76/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3975.7822 - mae: 50.4121 - val_loss: 852.7792 - val_mae: 23.2413\n",
      "Epoch 77/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3913.5000 - mae: 50.2110 - val_loss: 841.6248 - val_mae: 23.2978\n",
      "Epoch 78/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3992.6917 - mae: 50.2754 - val_loss: 862.0323 - val_mae: 23.3116\n",
      "Epoch 79/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4077.4260 - mae: 50.0566 - val_loss: 888.4592 - val_mae: 23.7624\n",
      "Epoch 80/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3680.5144 - mae: 48.2898 - val_loss: 773.3264 - val_mae: 21.9828\n",
      "Epoch 81/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4544.5312 - mae: 52.6628 - val_loss: 756.8914 - val_mae: 21.9658\n",
      "Epoch 82/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3565.2356 - mae: 46.5294 - val_loss: 782.9288 - val_mae: 22.4253\n",
      "Epoch 83/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4089.3079 - mae: 51.5675 - val_loss: 743.2680 - val_mae: 21.4962\n",
      "Epoch 84/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4084.3474 - mae: 51.5467 - val_loss: 731.4321 - val_mae: 21.1888\n",
      "Epoch 85/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4065.0527 - mae: 51.4129 - val_loss: 801.5359 - val_mae: 22.8976\n",
      "Epoch 86/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3992.8796 - mae: 50.9091 - val_loss: 810.4245 - val_mae: 22.8978\n",
      "Epoch 87/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3569.6421 - mae: 48.2094 - val_loss: 769.9440 - val_mae: 21.7719\n",
      "Epoch 88/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3418.7629 - mae: 46.9757 - val_loss: 737.8169 - val_mae: 21.6225\n",
      "Epoch 89/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3807.0398 - mae: 49.2202 - val_loss: 757.6072 - val_mae: 22.3053\n",
      "Epoch 90/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3920.7561 - mae: 51.2836 - val_loss: 804.2252 - val_mae: 22.3801\n",
      "Epoch 91/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3473.4946 - mae: 47.9597 - val_loss: 805.7424 - val_mae: 22.5413\n",
      "Epoch 92/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3779.6265 - mae: 49.1774 - val_loss: 743.0809 - val_mae: 21.5037\n",
      "Epoch 93/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3934.2344 - mae: 50.9082 - val_loss: 732.5540 - val_mae: 21.4214\n",
      "Epoch 94/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3428.9590 - mae: 45.2876 - val_loss: 699.9492 - val_mae: 20.7325\n",
      "Epoch 95/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3417.9600 - mae: 47.0958 - val_loss: 688.9009 - val_mae: 20.5166\n",
      "Epoch 96/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2851.9866 - mae: 43.2538 - val_loss: 720.6753 - val_mae: 21.0600\n",
      "Epoch 97/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3731.1423 - mae: 48.3936 - val_loss: 839.0309 - val_mae: 23.3527\n",
      "Epoch 98/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3116.8516 - mae: 45.4195 - val_loss: 722.9957 - val_mae: 21.3401\n",
      "Epoch 99/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3417.6458 - mae: 46.4344 - val_loss: 719.7700 - val_mae: 21.5079\n",
      "Epoch 100/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3731.7500 - mae: 47.9047 - val_loss: 742.6249 - val_mae: 21.4869\n",
      "Epoch 101/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3292.6938 - mae: 45.8391 - val_loss: 745.5974 - val_mae: 21.5190\n",
      "Epoch 102/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3778.7974 - mae: 48.6241 - val_loss: 695.1816 - val_mae: 20.8339\n",
      "Epoch 103/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3667.3401 - mae: 48.6982 - val_loss: 690.8091 - val_mae: 20.8146\n",
      "Epoch 104/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3471.0825 - mae: 46.4526 - val_loss: 730.2548 - val_mae: 21.7388\n",
      "Epoch 105/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3834.2192 - mae: 49.2920 - val_loss: 794.7283 - val_mae: 22.4343\n",
      "Epoch 106/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3182.1050 - mae: 45.0397 - val_loss: 799.7632 - val_mae: 22.5384\n",
      "Epoch 107/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4096.0615 - mae: 50.7168 - val_loss: 702.1848 - val_mae: 21.0228\n",
      "Epoch 108/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3418.1936 - mae: 46.3391 - val_loss: 704.9582 - val_mae: 21.0563\n",
      "Epoch 109/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3965.3289 - mae: 49.9783 - val_loss: 750.8028 - val_mae: 22.0081\n",
      "Epoch 110/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4049.1179 - mae: 50.6239 - val_loss: 754.3371 - val_mae: 22.2599\n",
      "Epoch 111/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3864.3970 - mae: 51.3261 - val_loss: 778.2369 - val_mae: 22.1467\n",
      "Epoch 112/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3672.4019 - mae: 47.7678 - val_loss: 834.3419 - val_mae: 23.1936\n",
      "Epoch 113/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3570.6609 - mae: 48.6424 - val_loss: 797.7210 - val_mae: 22.9697\n",
      "Epoch 114/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3839.3591 - mae: 48.5386 - val_loss: 739.7151 - val_mae: 21.6868\n",
      "Epoch 115/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2982.5156 - mae: 43.2994 - val_loss: 683.8205 - val_mae: 20.8065\n",
      "Epoch 116/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3712.5273 - mae: 47.3462 - val_loss: 659.9307 - val_mae: 20.3832\n",
      "Epoch 117/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3795.4045 - mae: 49.1951 - val_loss: 690.8805 - val_mae: 20.9417\n",
      "Epoch 118/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3783.4768 - mae: 47.3705 - val_loss: 779.4297 - val_mae: 22.2943\n",
      "Epoch 119/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3516.6682 - mae: 45.7118 - val_loss: 720.0705 - val_mae: 21.4474\n",
      "Epoch 120/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3974.3333 - mae: 50.0970 - val_loss: 665.6013 - val_mae: 20.4928\n",
      "Epoch 121/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3643.5254 - mae: 48.2739 - val_loss: 730.1298 - val_mae: 21.3558\n",
      "Epoch 122/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4206.1611 - mae: 51.1329 - val_loss: 689.3023 - val_mae: 20.7010\n",
      "Epoch 123/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3754.6038 - mae: 48.2707 - val_loss: 681.7942 - val_mae: 20.7533\n",
      "Epoch 124/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3386.0659 - mae: 46.1587 - val_loss: 710.0520 - val_mae: 21.0785\n",
      "Epoch 125/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3980.3442 - mae: 50.7001 - val_loss: 704.4318 - val_mae: 20.9675\n",
      "Epoch 126/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3248.8467 - mae: 44.6596 - val_loss: 691.7673 - val_mae: 20.9636\n",
      "Epoch 127/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3805.4050 - mae: 48.4892 - val_loss: 684.2874 - val_mae: 20.7813\n",
      "Epoch 128/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3204.1807 - mae: 45.1016 - val_loss: 756.6736 - val_mae: 21.7683\n",
      "Epoch 129/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3454.7397 - mae: 48.2386 - val_loss: 705.0311 - val_mae: 20.9234\n",
      "Epoch 130/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3455.7961 - mae: 47.1079 - val_loss: 700.4678 - val_mae: 21.0572\n",
      "Epoch 131/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3381.0586 - mae: 46.3860 - val_loss: 696.5798 - val_mae: 20.9365\n",
      "Epoch 132/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3030.2759 - mae: 44.2285 - val_loss: 751.8484 - val_mae: 21.7205\n",
      "Epoch 133/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3507.7397 - mae: 47.9696 - val_loss: 781.6696 - val_mae: 22.2364\n",
      "Epoch 134/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3510.4717 - mae: 47.3521 - val_loss: 679.8514 - val_mae: 20.7128\n",
      "Epoch 135/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3327.7405 - mae: 46.6913 - val_loss: 650.2762 - val_mae: 20.2364\n",
      "Epoch 136/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3761.9128 - mae: 49.9549 - val_loss: 721.5383 - val_mae: 21.3029\n",
      "Epoch 137/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3412.9741 - mae: 45.4647 - val_loss: 740.5609 - val_mae: 21.8493\n",
      "Epoch 138/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3438.1238 - mae: 46.0564 - val_loss: 707.8684 - val_mae: 21.0164\n",
      "Epoch 139/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3629.4829 - mae: 47.5269 - val_loss: 706.8737 - val_mae: 21.1540\n",
      "Epoch 140/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3610.7424 - mae: 47.6769 - val_loss: 650.7260 - val_mae: 20.2563\n",
      "Epoch 141/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3391.8511 - mae: 46.5246 - val_loss: 734.1372 - val_mae: 21.4711\n",
      "Epoch 142/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3516.9595 - mae: 47.1189 - val_loss: 772.6022 - val_mae: 22.1389\n",
      "Epoch 143/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3911.4875 - mae: 48.8396 - val_loss: 836.6533 - val_mae: 23.2067\n",
      "Epoch 144/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3965.0249 - mae: 51.9172 - val_loss: 853.3578 - val_mae: 23.3969\n",
      "Epoch 145/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3192.9624 - mae: 44.8643 - val_loss: 761.9952 - val_mae: 21.9587\n",
      "Epoch 146/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3520.6519 - mae: 46.5781 - val_loss: 717.4940 - val_mae: 21.3610\n",
      "Epoch 147/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3504.6465 - mae: 47.3614 - val_loss: 686.6101 - val_mae: 20.7610\n",
      "Epoch 148/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3140.3770 - mae: 44.0736 - val_loss: 724.5261 - val_mae: 21.2960\n",
      "Epoch 149/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3793.0791 - mae: 48.7540 - val_loss: 810.1422 - val_mae: 22.7723\n",
      "Epoch 150/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3196.5054 - mae: 45.4890 - val_loss: 681.0159 - val_mae: 20.7575\n",
      "Epoch 151/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3458.3350 - mae: 46.9935 - val_loss: 757.1928 - val_mae: 21.8612\n",
      "Epoch 152/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3118.5181 - mae: 44.9070 - val_loss: 767.4741 - val_mae: 22.0981\n",
      "Epoch 153/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3624.1389 - mae: 47.3921 - val_loss: 771.4039 - val_mae: 22.1523\n",
      "Epoch 154/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3252.5476 - mae: 45.5134 - val_loss: 753.4032 - val_mae: 21.7697\n",
      "Epoch 155/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3600.9065 - mae: 46.5379 - val_loss: 646.1962 - val_mae: 20.2229\n",
      "Epoch 156/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3452.6279 - mae: 47.9939 - val_loss: 669.8098 - val_mae: 20.6200\n",
      "Epoch 157/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3039.0403 - mae: 44.3963 - val_loss: 720.8448 - val_mae: 21.2422\n",
      "Epoch 158/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3726.4155 - mae: 48.7139 - val_loss: 656.4604 - val_mae: 20.3634\n",
      "Epoch 159/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2951.5400 - mae: 43.6317 - val_loss: 683.2504 - val_mae: 20.8517\n",
      "Epoch 160/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3373.1880 - mae: 46.4706 - val_loss: 688.2068 - val_mae: 20.9305\n",
      "Epoch 161/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4013.2603 - mae: 49.7393 - val_loss: 659.5130 - val_mae: 20.4527\n",
      "Epoch 162/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3446.0466 - mae: 48.3384 - val_loss: 755.2322 - val_mae: 21.8196\n",
      "Epoch 163/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3725.7678 - mae: 48.0852 - val_loss: 826.9269 - val_mae: 22.9726\n",
      "Epoch 164/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3613.2795 - mae: 47.5139 - val_loss: 837.1606 - val_mae: 23.0887\n",
      "Epoch 165/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3733.9966 - mae: 47.3829 - val_loss: 717.2693 - val_mae: 21.4833\n",
      "Epoch 166/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3395.8230 - mae: 46.1414 - val_loss: 722.4512 - val_mae: 21.5148\n",
      "Epoch 167/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3681.8188 - mae: 48.3793 - val_loss: 873.6105 - val_mae: 23.5924\n",
      "Epoch 168/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3472.0840 - mae: 47.0813 - val_loss: 730.1335 - val_mae: 21.4095\n",
      "Epoch 169/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3442.8254 - mae: 48.1893 - val_loss: 725.7430 - val_mae: 21.7663\n",
      "Epoch 170/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3783.2742 - mae: 49.0231 - val_loss: 702.2130 - val_mae: 21.2408\n",
      "Epoch 171/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3224.9114 - mae: 46.4372 - val_loss: 826.9240 - val_mae: 23.0929\n",
      "Epoch 172/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3326.1353 - mae: 46.5447 - val_loss: 737.6382 - val_mae: 21.7327\n",
      "Epoch 173/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3733.0996 - mae: 49.2987 - val_loss: 681.4312 - val_mae: 20.9289\n",
      "Epoch 174/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3887.3284 - mae: 48.6967 - val_loss: 727.6571 - val_mae: 21.8199\n",
      "Epoch 175/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3332.5186 - mae: 46.4960 - val_loss: 708.2139 - val_mae: 21.1533\n",
      "Epoch 176/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3283.3701 - mae: 45.9670 - val_loss: 722.8449 - val_mae: 21.6822\n",
      "Epoch 177/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3976.3386 - mae: 50.2082 - val_loss: 783.8604 - val_mae: 22.3168\n",
      "Epoch 178/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3383.4438 - mae: 46.5211 - val_loss: 725.9107 - val_mae: 21.5172\n",
      "Epoch 179/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3232.0061 - mae: 45.2834 - val_loss: 680.3232 - val_mae: 20.8587\n",
      "Epoch 180/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2988.5061 - mae: 43.4167 - val_loss: 885.8405 - val_mae: 23.9044\n",
      "Epoch 181/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3738.0476 - mae: 48.5604 - val_loss: 754.6415 - val_mae: 21.8278\n",
      "Epoch 182/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3563.1221 - mae: 48.1235 - val_loss: 668.6997 - val_mae: 20.6047\n",
      "Epoch 183/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3766.3586 - mae: 49.0755 - val_loss: 759.1011 - val_mae: 22.2543\n",
      "Epoch 184/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3489.8606 - mae: 45.1922 - val_loss: 673.5472 - val_mae: 20.7502\n",
      "Epoch 185/300\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3145.8853 - mae: 45.2054 - val_loss: 818.5926 - val_mae: 22.8346\n",
      "Test MAE: 22.43\n",
      "Test MSE: 936.59\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "Test R-squared: 0.8730\n"
     ]
    }
   ],
   "source": [
    "# X_selected is your features after selection, shape (n_samples, 4)\n",
    "\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Split the selected features\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_selected, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build model (simple architecture)\n",
    "model = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=30,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=300,\n",
    "    batch_size=16,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "loss, mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test MAE: {mae:.2f}\")\n",
    "print(f\"Test MSE: {loss:.2f}\")\n",
    "\n",
    "# Predict and calculate R2\n",
    "y_pred = model.predict(X_test).flatten()\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Test R-squared: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d5e46be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with learning rate=0.001, dropout=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gauta\\OneDrive\\Dokumentai\\cattle_model\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "R-squared: 0.8633\n",
      "\n",
      "Training with learning rate=0.001, dropout=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gauta\\OneDrive\\Dokumentai\\cattle_model\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "R-squared: 0.8687\n",
      "\n",
      "Training with learning rate=0.0005, dropout=0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gauta\\OneDrive\\Dokumentai\\cattle_model\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "R-squared: 0.8477\n",
      "\n",
      "Training with learning rate=0.0005, dropout=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gauta\\OneDrive\\Dokumentai\\cattle_model\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "R-squared: 0.8666\n",
      "\n",
      "Best R-squared: 0.8687 with params: {'learning_rate': 0.001, 'dropout': 0.5}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "learning_rates = [0.001, 0.0005]\n",
    "dropout_rates = [0.3, 0.5]\n",
    "best_r2 = -float('inf')\n",
    "best_params = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for dr in dropout_rates:\n",
    "        print(f\"Training with learning rate={lr}, dropout={dr}\")\n",
    "        model = models.Sequential([\n",
    "            layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "            layers.Dropout(dr),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(lr), loss='mse', metrics=['mae'])\n",
    "\n",
    "        early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_split=0.2,\n",
    "            epochs=300,\n",
    "            batch_size=16,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        y_pred = model.predict(X_test).flatten()\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        print(f\"R-squared: {r2:.4f}\\n\")\n",
    "\n",
    "        if r2 > best_r2:\n",
    "            best_r2 = r2\n",
    "            best_params = {'learning_rate': lr, 'dropout': dr}\n",
    "\n",
    "print(f\"Best R-squared: {best_r2:.4f} with params: {best_params}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
